import streamlit as st
import pandas as pd
import os
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="An√°lise de Dados - Transcri√ß√µes",
    page_icon="üìä",
    layout="wide"
)

# T√≠tulo da p√°gina
st.title("An√°lise de Dados das Transcri√ß√µes")
st.markdown("### Visualize insights e estat√≠sticas das transcri√ß√µes de reuni√µes")

# Diret√≥rio de sa√≠da
output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "saidas")

# Fun√ß√£o para extrair informa√ß√µes do nome do arquivo
def extract_info_from_filename(filename):
    # Padr√£o para extrair informa√ß√µes do nome do arquivo
    pattern = r"(html|excel)_(.+)\.(html|xlsx)"
    match = re.match(pattern, filename)
    if match:
        file_type = match.group(1)
        meeting_name = match.group(2)
        return {
            "type": file_type,
            "meeting_name": meeting_name
        }
    return None

# Fun√ß√£o para ler arquivos Excel
def get_excel_data(file_path, sheet_name=None):
    """Carrega dados de um arquivo Excel.
    
    Args:
        file_path: Caminho para o arquivo Excel
        sheet_name: Nome da planilha espec√≠fica para carregar. 
                    Se None, carrega todas as planilhas.
    
    Returns:
        Se sheet_name for None, retorna um dicion√°rio com todas as planilhas.
        Se sheet_name for especificado, retorna um DataFrame com os dados da planilha.
    """
    if sheet_name is None:
        # Retorna um dicion√°rio com todas as planilhas
        return pd.read_excel(file_path, sheet_name=None)
    else:
        # Retorna uma planilha espec√≠fica
        return pd.read_excel(file_path, sheet_name=sheet_name)

# Fun√ß√£o para baixar NLTK stopwords se necess√°rio
@st.cache_resource
def download_nltk_resources():
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')

# Baixar recursos NLTK
download_nltk_resources()

# Fun√ß√£o para processar texto e remover stopwords
def process_text(text, language='portuguese'):
    # Tokenizar o texto
    tokens = word_tokenize(text.lower(), language=language)
    
    # Remover stopwords
    stop_words = set(stopwords.words(language))
    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    
    return filtered_tokens

# Fun√ß√£o para criar nuvem de palavras
def create_wordcloud(text, title="Nuvem de Palavras"):
    from wordcloud import WordCloud
    
    # Criar nuvem de palavras
    wordcloud = WordCloud(width=800, height=400, background_color='white', 
                         colormap='viridis', max_words=100).generate(text)
    
    # Plotar a nuvem de palavras
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.set_title(title, fontsize=15)
    ax.axis('off')
    
    return fig

# Fun√ß√£o para analisar sentimentos (simplificada)
def analyze_sentiment(text):
    # Esta √© uma an√°lise de sentimento muito simplificada
    # Em um cen√°rio real, usar√≠amos uma biblioteca como TextBlob, VADER ou um modelo de ML
    
    # Lista de palavras positivas e negativas em portugu√™s
    positive_words = ['bom', '√≥timo', 'excelente', 'positivo', 'feliz', 'concordo', 'aprovado', 
                     'sucesso', 'eficiente', 'eficaz', 'melhor', 'progresso', 'avan√ßo']
    
    negative_words = ['ruim', 'p√©ssimo', 'negativo', 'triste', 'discordo', 'reprovado', 
                     'fracasso', 'ineficiente', 'ineficaz', 'pior', 'problema', 'dificuldade']
    
    # Tokenizar o texto
    tokens = word_tokenize(text.lower(), language='portuguese')
    
    # Contar palavras positivas e negativas
    positive_count = sum(1 for word in tokens if word in positive_words)
    negative_count = sum(1 for word in tokens if word in negative_words)
    
    # Calcular pontua√ß√£o de sentimento
    total = positive_count + negative_count
    if total == 0:
        return 0  # Neutro
    
    sentiment_score = (positive_count - negative_count) / total
    return sentiment_score

# Listar arquivos Excel
excel_files = [f for f in os.listdir(output_dir) if f.endswith('.xlsx')]
excel_files.sort()

# Sidebar para sele√ß√£o de arquivo
selected_excel = st.sidebar.selectbox("Selecione um arquivo para an√°lise", excel_files)

if selected_excel:
    file_path = os.path.join(output_dir, selected_excel)
    
    # Extrair informa√ß√µes do nome do arquivo
    file_info = extract_info_from_filename(selected_excel)
    if file_info:
        meeting_name = file_info["meeting_name"]
        st.subheader(f"An√°lise de: {meeting_name}")
    else:
        st.subheader(f"An√°lise de: {selected_excel}")
    
    # Carregar dados do Excel
    try:
        # Carregar todas as planilhas do arquivo Excel
        dfs = get_excel_data(file_path)
        
        # Selecionar a primeira planilha para an√°lise
        first_sheet = list(dfs.keys())[0]
        df = dfs[first_sheet]
        
        # Exibir informa√ß√µes sobre o dataframe
        st.info(f"Analisando a planilha '{first_sheet}' com {df.shape[0]} linhas e {df.shape[1]} colunas")
        
        # Verificar se o dataframe tem as colunas esperadas
        expected_columns = ['timestamp', 'speaker', 'text']
        has_expected_columns = all(col in df.columns for col in expected_columns)
        
        if has_expected_columns:
            # An√°lises e visualiza√ß√µes
            st.markdown("## üìä An√°lises e Visualiza√ß√µes")
            
            # Criar abas para diferentes tipos de an√°lise
            tab1, tab2, tab3, tab4 = st.tabs(["üìù Vis√£o Geral", "üë• An√°lise de Participa√ß√£o", 
                                             "üî§ An√°lise de Texto", "üìà Tend√™ncias e Padr√µes"])
            
            with tab1:
                st.markdown("### Vis√£o Geral da Reuni√£o")
                
                # Estat√≠sticas b√°sicas
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    total_speakers = df['speaker'].nunique()
                    st.metric("Total de Participantes", total_speakers)
                
                with col2:
                    total_utterances = len(df)
                    st.metric("Total de Falas", total_utterances)
                
                with col3:
                    # Calcular dura√ß√£o aproximada da reuni√£o
                    if 'timestamp' in df.columns and len(df) > 0:
                        try:
                            # Tentar converter para datetime se for string
                            if isinstance(df['timestamp'].iloc[0], str):
                                df['timestamp'] = pd.to_datetime(df['timestamp'])
                            
                            duration = df['timestamp'].max() - df['timestamp'].min()
                            duration_minutes = duration.total_seconds() / 60
                            st.metric("Dura√ß√£o Aproximada", f"{duration_minutes:.1f} min")
                        except:
                            st.metric("Dura√ß√£o", "N√£o dispon√≠vel")
                    else:
                        st.metric("Dura√ß√£o", "N√£o dispon√≠vel")
                
                # Gr√°fico de distribui√ß√£o de falas ao longo do tempo
                st.markdown("#### Distribui√ß√£o de Falas ao Longo do Tempo")
                
                try:
                    # Criar um histograma de falas ao longo do tempo
                    if 'timestamp' in df.columns:
                        fig = px.histogram(df, x='timestamp', nbins=20, 
                                         title="Distribui√ß√£o de Falas ao Longo da Reuni√£o")
                        fig.update_layout(xaxis_title="Tempo", yaxis_title="N√∫mero de Falas")
                        st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.error(f"N√£o foi poss√≠vel criar o gr√°fico de distribui√ß√£o temporal: {e}")
            
            with tab2:
                st.markdown("### An√°lise de Participa√ß√£o")
                
                # Contagem de falas por participante
                speaker_counts = df['speaker'].value_counts().reset_index()
                speaker_counts.columns = ['Participante', 'N√∫mero de Falas']
                
                # Gr√°fico de barras de participa√ß√£o
                fig = px.bar(speaker_counts, x='Participante', y='N√∫mero de Falas',
                           title="N√∫mero de Falas por Participante",
                           color='N√∫mero de Falas', color_continuous_scale='Viridis')
                st.plotly_chart(fig, use_container_width=True)
                
                # An√°lise de tempo de fala por participante
                st.markdown("#### Tempo de Fala por Participante")
                
                # Calcular o comprimento do texto como proxy para tempo de fala
                df['text_length'] = df['text'].str.len()
                speaker_text_length = df.groupby('speaker')['text_length'].sum().reset_index()
                speaker_text_length.columns = ['Participante', 'Comprimento do Texto']
                
                # Normalizar para percentual
                total_length = speaker_text_length['Comprimento do Texto'].sum()
                speaker_text_length['Percentual'] = (speaker_text_length['Comprimento do Texto'] / total_length * 100).round(1)
                
                # Gr√°fico de pizza de tempo de fala
                fig = px.pie(speaker_text_length, values='Percentual', names='Participante',
                           title="Distribui√ß√£o do Tempo de Fala (%)",
                           hover_data=['Comprimento do Texto'])
                fig.update_traces(textposition='inside', textinfo='percent+label')
                st.plotly_chart(fig, use_container_width=True)
                
                # Tabela de estat√≠sticas de participa√ß√£o
                st.markdown("#### Estat√≠sticas Detalhadas de Participa√ß√£o")
                
                # Juntar contagem de falas e comprimento de texto
                participation_stats = pd.merge(speaker_counts, speaker_text_length, on='Participante')
                participation_stats['M√©dia de Caracteres por Fala'] = (participation_stats['Comprimento do Texto'] / 
                                                                    participation_stats['N√∫mero de Falas']).round(1)
                
                # Ordenar por n√∫mero de falas
                participation_stats = participation_stats.sort_values('N√∫mero de Falas', ascending=False)
                
                # Exibir tabela
                st.dataframe(participation_stats)
            
            with tab3:
                st.markdown("### An√°lise de Texto")
                
                # Combinar todo o texto para an√°lise
                all_text = ' '.join(df['text'].dropna())
                
                # Estat√≠sticas b√°sicas de texto
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    total_words = len(all_text.split())
                    st.metric("Total de Palavras", f"{total_words:,}")
                
                with col2:
                    unique_words = len(set(all_text.lower().split()))
                    st.metric("Palavras √önicas", f"{unique_words:,}")
                
                with col3:
                    lexical_diversity = unique_words / total_words if total_words > 0 else 0
                    st.metric("Diversidade Lexical", f"{lexical_diversity:.2f}")
                
                # Processamento de texto para an√°lise
                processed_tokens = process_text(all_text)
                word_freq = Counter(processed_tokens)
                
                # Palavras mais frequentes
                st.markdown("#### Palavras Mais Frequentes")
                
                # Obter as 20 palavras mais comuns
                most_common_words = word_freq.most_common(20)
                words_df = pd.DataFrame(most_common_words, columns=['Palavra', 'Frequ√™ncia'])
                
                # Gr√°fico de barras horizontais
                fig = px.bar(words_df, y='Palavra', x='Frequ√™ncia', orientation='h',
                           title="20 Palavras Mais Frequentes",
                           color='Frequ√™ncia', color_continuous_scale='Viridis')
                fig.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
                
                # Nuvem de palavras
                st.markdown("#### Nuvem de Palavras")
                
                try:
                    # Criar texto para nuvem de palavras
                    wordcloud_text = ' '.join(processed_tokens)
                    
                    # Gerar nuvem de palavras
                    fig = create_wordcloud(wordcloud_text, title="Nuvem de Palavras da Reuni√£o")
                    st.pyplot(fig)
                except Exception as e:
                    st.error(f"N√£o foi poss√≠vel criar a nuvem de palavras: {e}")
                    st.info("Instale a biblioteca wordcloud com 'pip install wordcloud' para habilitar esta funcionalidade.")
                
                # An√°lise de sentimento por participante
                st.markdown("#### An√°lise de Sentimento por Participante")
                
                # Calcular sentimento para cada fala
                df['sentiment'] = df['text'].apply(analyze_sentiment)
                
                # Calcular sentimento m√©dio por participante
                sentiment_by_speaker = df.groupby('speaker')['sentiment'].mean().reset_index()
                sentiment_by_speaker.columns = ['Participante', 'Sentimento M√©dio']
                
                # Gr√°fico de barras de sentimento
                fig = px.bar(sentiment_by_speaker, x='Participante', y='Sentimento M√©dio',
                           title="Sentimento M√©dio por Participante",
                           color='Sentimento M√©dio', color_continuous_scale='RdBu',
                           range_color=[-1, 1])
                fig.update_layout(yaxis_range=[-1, 1])
                st.plotly_chart(fig, use_container_width=True)
                
                st.info("Nota: Esta √© uma an√°lise de sentimento simplificada. Para resultados mais precisos, seria necess√°rio utilizar modelos de NLP mais avan√ßados.")
            
            with tab4:
                st.markdown("### Tend√™ncias e Padr√µes")
                
                # An√°lise de intera√ß√µes entre participantes
                st.markdown("#### Padr√£o de Intera√ß√µes")
                
                # Criar uma sequ√™ncia de falantes
                speaker_sequence = df['speaker'].tolist()
                
                # Contar transi√ß√µes entre falantes
                transitions = []
                for i in range(len(speaker_sequence) - 1):
                    transitions.append((speaker_sequence[i], speaker_sequence[i+1]))
                
                transition_counts = Counter(transitions)
                
                # Criar matriz de adjac√™ncia para visualiza√ß√£o
                unique_speakers = sorted(df['speaker'].unique())
                n_speakers = len(unique_speakers)
                
                # Criar dataframe para heatmap
                interaction_matrix = pd.DataFrame(0, index=unique_speakers, columns=unique_speakers)
                
                for (speaker1, speaker2), count in transition_counts.items():
                    interaction_matrix.loc[speaker1, speaker2] = count
                
                # Criar heatmap
                fig = px.imshow(interaction_matrix, 
                              labels=dict(x="Pr√≥ximo Falante", y="Falante Atual", color="N√∫mero de Transi√ß√µes"),
                              x=unique_speakers,
                              y=unique_speakers,
                              color_continuous_scale="Viridis")
                
                fig.update_layout(title="Padr√£o de Intera√ß√µes entre Participantes")
                st.plotly_chart(fig, use_container_width=True)
                
                # An√°lise de t√≥picos ao longo do tempo (simplificada)
                st.markdown("#### Evolu√ß√£o de Temas ao Longo da Reuni√£o")
                
                # Dividir a reuni√£o em segmentos
                n_segments = 5
                df_segments = [df.iloc[i*len(df)//n_segments:(i+1)*len(df)//n_segments] for i in range(n_segments)]
                
                # Extrair palavras-chave para cada segmento
                segment_keywords = []
                
                for i, segment in enumerate(df_segments):
                    # Combinar texto do segmento
                    segment_text = ' '.join(segment['text'].dropna())
                    
                    # Processar texto
                    segment_tokens = process_text(segment_text)
                    segment_freq = Counter(segment_tokens)
                    
                    # Obter as 5 palavras mais comuns
                    top_words = [word for word, _ in segment_freq.most_common(5)]
                    
                    # Adicionar ao resultado
                    segment_keywords.append({
                        'Segmento': f"Segmento {i+1}",
                        'Palavras-chave': ", ".join(top_words)
                    })
                
                # Criar dataframe
                keywords_df = pd.DataFrame(segment_keywords)
                
                # Exibir tabela
                st.table(keywords_df)
                
                # An√°lise de complexidade do discurso
                st.markdown("#### Complexidade do Discurso por Participante")
                
                # Calcular comprimento m√©dio das frases por participante
                df['sentence_count'] = df['text'].apply(lambda x: len(re.split(r'[.!?]+', x)))
                df['words_per_sentence'] = df['text'].apply(lambda x: len(x.split()) / max(1, len(re.split(r'[.!?]+', x))))
                
                # Agrupar por participante
                complexity_by_speaker = df.groupby('speaker')['words_per_sentence'].mean().reset_index()
                complexity_by_speaker.columns = ['Participante', 'Palavras por Frase']
                
                # Gr√°fico de barras
                fig = px.bar(complexity_by_speaker, x='Participante', y='Palavras por Frase',
                           title="Complexidade do Discurso por Participante",
                           color='Palavras por Frase', color_continuous_scale='Viridis')
                st.plotly_chart(fig, use_container_width=True)
        
        else:
            st.warning("O formato do arquivo n√£o corresponde ao esperado para an√°lise. Verifique se o arquivo cont√©m as colunas: 'timestamp', 'speaker' e 'text'.")
            
            # Mostrar as colunas dispon√≠veis
            st.info(f"Colunas dispon√≠veis no arquivo: {', '.join(df.columns)}")
            
            # Exibir uma amostra dos dados
            st.subheader("Amostra dos Dados")
            st.dataframe(df.head())
    
    except Exception as e:
        st.error(f"Erro ao carregar ou analisar o arquivo Excel: {e}")
        st.exception(e)

else:
    st.info("Selecione um arquivo Excel no menu lateral para come√ßar a an√°lise.")

# Rodap√©
st.sidebar.markdown("---")
st.sidebar.info("Esta p√°gina permite analisar os dados das transcri√ß√µes de reuni√µes.")