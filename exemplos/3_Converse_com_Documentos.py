import streamlit as st
import pandas as pd
import os
import re
import google.generativeai as genai

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Converse com Documentos - Transcri√ß√µes",
    page_icon="üí¨",
    layout="wide"
)

# T√≠tulo da p√°gina
st.title("Converse com os Documentos")
st.markdown("### Fa√ßa perguntas sobre o conte√∫do das transcri√ß√µes de reuni√µes")
st.markdown("Utilize a caixa de texto abaixo para interagir com o conte√∫do das suas transcri√ß√µes. Voc√™ pode fazer perguntas espec√≠ficas, pedir resumos ou buscar informa√ß√µes detalhadas. Por exemplo, tente perguntar: 'Qual foi o principal t√≥pico da reuni√£o de 09/08?', 'Quem participou da reuni√£o de 16/08?', ou 'Resuma as decis√µes tomadas em todas as reuni√µes'.")

# Diret√≥rio de sa√≠da
output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "saidas")

# Fun√ß√£o para extrair informa√ß√µes do nome do arquivo
def extract_info_from_filename(filename):
    # Padr√£o para extrair informa√ß√µes do nome do arquivo
    pattern = r"(html|excel)_(.+)\.(html|xlsx)"
    match = re.match(pattern, filename)
    if match:
        file_type = match.group(1)
        meeting_name = match.group(2)
        return {
            "type": file_type,
            "meeting_name": meeting_name
        }
    return None

# Fun√ß√£o para ler arquivos HTML
def get_html_content(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

# Fun√ß√£o para extrair texto limpo do HTML
def extract_text_from_html(html_content):
    # Remover tags HTML (implementa√ß√£o simplificada)
    # Em um cen√°rio real, usar√≠amos BeautifulSoup ou similar
    text = re.sub(r'<.*?>', ' ', html_content)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# Fun√ß√£o para configurar a API do Gemini
def configure_genai(api_key):
    genai.configure(api_key=api_key)

# Fun√ß√£o para carregar e processar todos os documentos
@st.cache_data
def load_all_documents():
    documents = {}
    html_files = [f for f in os.listdir(output_dir) if f.endswith('.html')]
    
    for html_file in html_files:
        file_path = os.path.join(output_dir, html_file)
        file_info = extract_info_from_filename(html_file)
        
        if file_info:
            meeting_name = file_info["meeting_name"]
            html_content = get_html_content(file_path)
            text_content = extract_text_from_html(html_content)
            
            documents[meeting_name] = {
                "filename": html_file,
                "content": text_content,
                "path": file_path
            }
    
    return documents

# Fun√ß√£o para responder perguntas com Gemini
def answer_question(model, question, context, meeting_name):
    # Construir o prompt
    prompt = f"""Voc√™ √© um assistente especializado em analisar transcri√ß√µes de reuni√µes. 
    Responda √† pergunta com base apenas nas informa√ß√µes contidas na transcri√ß√£o fornecida.
    Se a resposta n√£o estiver na transcri√ß√£o, diga claramente que n√£o consegue responder com base nas informa√ß√µes dispon√≠veis.
    
    Transcri√ß√£o da reuni√£o '{meeting_name}':
    {context[:15000]}
    
    Pergunta: {question}
    
    Resposta:"""
    
    try:
        # Gerar resposta com o modelo Gemini
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        st.error(f"Erro ao gerar resposta: {e}")
        return f"Ocorreu um erro ao processar sua pergunta: {str(e)}"

# Fun√ß√£o para responder perguntas com m√∫ltiplos documentos
def answer_with_multiple_documents(model, question, documents, selected_docs):
    # Construir contexto combinado
    combined_context = ""
    for doc_name in selected_docs:
        if doc_name in documents:
            doc_info = documents[doc_name]
            # Limitar o tamanho de cada documento para n√£o exceder limites de tokens
            doc_content = doc_info["content"][:5000]  # Primeiros 5000 caracteres de cada documento
            combined_context += f"\n\nTranscri√ß√£o da reuni√£o '{doc_name}':\n{doc_content}\n"
    
    # Construir o prompt
    prompt = f"""Voc√™ √© um assistente especializado em analisar transcri√ß√µes de reuni√µes. 
    Responda √† pergunta com base apenas nas informa√ß√µes contidas nas transcri√ß√µes fornecidas.
    Se a resposta n√£o estiver nas transcri√ß√µes, diga claramente que n√£o consegue responder com base nas informa√ß√µes dispon√≠veis.
    Quando a informa√ß√£o estiver em uma transcri√ß√£o espec√≠fica, mencione qual reuni√£o cont√©m essa informa√ß√£o.
    
    Contexto das transcri√ß√µes:
    {combined_context}
    
    Pergunta: {question}
    
    Resposta:"""
    
    try:
        # Gerar resposta com o modelo Gemini
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        st.error(f"Erro ao gerar resposta: {e}")
        return f"Ocorreu um erro ao processar sua pergunta: {str(e)}"

# Interface principal

# Inicializar hist√≥rico de chat se n√£o existir
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# Inicializar chave API se n√£o existir
if 'gemini_api_key' not in st.session_state:
    st.session_state.gemini_api_key = ""

# Configura√ß√£o da API Gemini usando secrets
try:
    api_key = st.secrets["api_key"]
    configure_genai(api_key)
except Exception as e:
    st.error("Chave da API Gemini n√£o encontrada. Por favor, configure-a no arquivo .streamlit/secrets.toml")
    st.stop()

# Carregar documentos
documents = load_all_documents()

# Sempre usar todos os documentos
selected_docs = list(documents.keys())

# Fun√ß√£o para processar a pergunta
def process_question(question, selected_docs, documents):
    if question and selected_docs and st.session_state.gemini_api_key:
        # Mostrar spinner durante o processamento
        with st.spinner("Processando sua pergunta..."):
            try:
                # Configurar modelo Gemini
                model = genai.GenerativeModel('gemini-2.5-flash')
                
                # Gerar resposta
                if len(selected_docs) == 1:
                    # Resposta com um √∫nico documento
                    doc_name = selected_docs[0]
                    doc_info = documents[doc_name]
                    response = answer_question(model, question, doc_info["content"], doc_name)
                else:
                    # Resposta com m√∫ltiplos documentos
                    response = answer_with_multiple_documents(model, question, documents, selected_docs)
                
                # Adicionar resposta ao hist√≥rico
                st.session_state.chat_history.append({"role": "assistant", "content": response})
            
            except Exception as e:
                st.session_state.chat_history.append({"role": "assistant", "content": f"Erro: {str(e)}"})



user_question = st.chat_input(placeholder="Digite sua pergunta aqui...")
if user_question:
    st.session_state.chat_history.append({"role": "user", "content": user_question})
    process_question(user_question, selected_docs, documents)